---
title: "Sotera Twitter Analysis"
author: "Jeffrey Uslan"
date: "October 5, 2015"
output: ioslides_presentation
---


##Synopsis

The goal of this analysis is to identify a single twitter user with a high volume of tweets which are also geotagged. The data has been acquired using the "streamR" package in the R statistical programming environment. This package allows are to interface with the twitter API. 

The processed tweets will be divided into 100 bins according to latitudinal and longitudinal coordinates. Probabilities will be assigned simply by percent of tweets submitted from that grid location.




```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(streamR)
library(Rfacebook)
library(smappR)
library(ROAuth)
library(ggplot2)
library(RCurl)
library(httr)
library(twitteR)
library(grid)
library(maps)
library(dplyr)
library(tidyr)
library(pander)

```

##Identifying a User

The top 3 followed twitter users without political ramifications were considered (President Barack Obama is the 3 most followed user at the writing of this report). The users to be considered are Katy Perry, Justin Bieber, and Taylor Sqift. 

```{r pander , echo=FALSE}
potential_users=c("katyperry","justinbieber","taylorswift13")
user_ids=c("21447363","27260086","17919972")
potential_user.df=data.frame(potential_users,user_ids)
pander(potential_user.df)
```

## Sampling Queries

Before commiting computational resources a sampling of each potential subject is taken. This takes the form of 20 minutes of access with the twitter API. For development purposes it is critically important that the owner of the application is a follower of any queried user. 

```{r , eval=FALSE, echo=FALSE}
for (i in 1:nrow(potential_user.df)){
  user=as.character(potential_user.df$potential_users[i])
  user_id=as.character(potential_user.df$user_ids[i])
  
  tweets2.df=data.frame()
  
  minutes=20
  for (i in 1:minutes){
    try(file.remove("tweets2.json"))
    filterStream("tweets2.json", timeout = 60, follow=c(user_id),
                 oauth = my_oauth)
    tmp<-try(parseTweets("tweets2.json", verbose  = FALSE))
    if (class(tmp) != "try-error") tweets2.df <- rbind(tweets2.df,tmp)
    if (sum(duplicated(tweets2.df))>0) tweets2.df=tweets2.df[!duplicated(tweets2.df),]
  }
  save(tweets2.df, file=paste0("./",user,"2.rda"))
}
```


```{r echo=FALSE}

tweet_tot_n=NULL
tweet_geo_n=NULL
tweet_geo_lat_sd=NULL
tweet_geo_lon_sd=NULL
for (i in 1:nrow(potential_user.df)){
  user=as.character(potential_user.df$potential_users[i])
  user_id=as.character(potential_user.df$user_ids[i])
  load(paste0("./",user,"2.rda"))
  
  tweet_tot_n=c(tweet_tot_n,nrow(tweets2.df))
  tweet_geo_n=c(tweet_geo_n,sum(!is.na(tweets2.df$place_lat)))
  tweet_geo_lat_sd=c(tweet_geo_lat_sd,round(sd(tweets2.df$place_lat,na.rm=TRUE),2))
  tweet_geo_lon_sd=c(tweet_geo_lon_sd,round(sd(tweets2.df$place_lon,na.rm=TRUE),2))
  
}
potential_user.df=data.frame(potential_user.df,
                             Samples=tweet_tot_n,
                             Geo_Tagged=tweet_geo_n,
                             Lat_SD=tweet_geo_lat_sd,
                             Lon_SD=tweet_geo_lon_sd)

pander(potential_user.df[,c(1,3:6)])
```

From this table we decide to proceed with Taylor Swift as the subject of analysis. She has the most geotagged tweets and the greatest geographic variability.


## Gathering Data
Now that Taylor Swift has been identified as the ideal analysis subject we can dedicate more resources to gathering her data. This will take the form of  3 hours of wall clock time to gather data. The twitter API is highly sensitive to unstable bandwidth so it is recommonded that short queries be submitted and saved periodically. 

```{r , eval=FALSE, echo=FALSE}
user="taylorswift13"
user_id="17919972"

tweets3.df=data.frame()
minutes=180
for (i in 1:minutes){
  try(file.remove("tweets3.json"))
  filterStream("tweets3.json", timeout = 60, follow=c(user_id),
               oauth = my_oauth)
  tmp<-try(parseTweets("tweets3.json", verbose  = FALSE))
  if (class(tmp) != "try-error") tweets3.df <- rbind(tweets3.df,tmp)
  if (sum(duplicated(tweets3.df))>0) tweets3.df=tweets3.df[!duplicated(tweets3.df),]
}
```


```{r, eval=FALSE, echo=FALSE}
save(tweets3.df, file=paste0("./",user,"3.rda"))

load(paste0("./",user,"2.rda"))

final_data=rbind(tweets2.df,tweets3.df)
if (sum(duplicated(final_data))>0) final_data=final_data[!duplicated(final_data),]
save(final_data, file=paste0("./final_data.rda"))
```

```{r, echo=FALSE}
load(paste0("./final_data.rda"))
```

##The Final Data

The final dataset has 1211 tweets, 53 of which are geotagged. All other tweets are removed for the analysis.

```{r, echo=FALSE}
final_data=final_data[!is.na(final_data$place_lat),]
final_data=final_data[!is.na(final_data$place_lon),]
```


```{r echo=FALSE}
final_data=tbl_df(final_data)
pander(final_data %>% summarise(Min_Latitude=round(min(place_lat),1),
                         Max_Latitude=round(max(place_lat),1),
                         SD_Latitude=round(sd(place_lat),1)))
                         
pander(final_data %>% summarise(Min_Longitude=round(min(place_lon),1),
                         Max_Longitude=round(max(place_lon),1),
                         SD_Longitude=round(sd(place_lon),1)))
```



## Distribution Plot
The plot below shows the origin locations of the 53 tweets in this study

```{r, echo=FALSE}
map.data <- map_data(map="world")
points <- data.frame(x = as.numeric(final_data$place_lon), y = as.numeric(final_data$place_lat))
ggplot(map.data) + 
  geom_map(aes(map_id = region), map = map.data, fill = "white",color = "grey20", size = 0.25) + 
  geom_point(data = points,aes(x = x, y = y), size = 7, alpha = 1/5, color = "darkblue") +
  ggtitle("Locations of Taylor Swift's Tweets")+xlab("Longitude")+ylab("Latitude")
```



```{r, echo=FALSE}
grids_x=10
lon_min=floor(min(final_data$place_lon))
lon_max=ceiling(max(final_data$place_lon))
grid_lon=seq(lon_min,lon_max,((lon_max-lon_min)/grids_x))

lat_min=floor(min(final_data$place_lat))
lat_max=ceiling(max(final_data$place_lat))
grid_lat=seq(lat_min,lat_max,((lat_max-lat_min)/grids_x))

final_data$lon_grid=NA
final_data$lat_grid=NA
for (i in 2:(grids_x+1)){
  lon_ind=which(final_data$place_lon<=grid_lon[i] & final_data$place_lon>=grid_lon[i-1])
  final_data$lon_grid[lon_ind]=(i-1)
  lat_ind=which(final_data$place_lat<=grid_lat[i] & final_data$place_lat>=grid_lat[i-1])
  final_data$lat_grid[lat_ind]=(i-1)
}

final_data$Grid=paste0("(",final_data$lat_grid,",",final_data$lon_grid,")")
```



##Grid Distributions
The following table shows the distibution of paired grid locations.
```{r  echo=FALSE}
pander(head(final_data %>% group_by(Grid) %>% summarise(Count=n(),
                                Probability=round((n()/nrow(final_data)),2),                  
                                Longitude_Mean=round(mean(place_lon),1),
                                Latitude_Mean=round(mean(place_lat),1)) %>% 
                      arrange(desc(Probability))))

```



##Longitudinal Distribution
The following table shows the distributions longitudinal ordinates.
```{r, echo=FALSE}
pander(head(final_data %>% group_by(lon_grid) %>% summarise(Count=n(),
                                                Probability=round(n()/nrow(final_data),2),              
                                                Longitude_Mean=round(mean(place_lon),1)) %>% 
  arrange(desc(Probability))))
```


##Latitudinal Distribution
The following table shows the distributions of latitudinal ordinates.

```{r , echo=FALSE}
pander(head(final_data %>% group_by(lat_grid) %>% summarise(Count=n(),
                                                Probability=round(n()/nrow(final_data),2),              
                                                Latitude_Mean=round(min(place_lat),1)) %>% 
  arrange(desc(Probability))))
```


